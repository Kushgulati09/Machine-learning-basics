{
   "metadata":{
      "kernelspec":{
         "language":"python",
         "display_name":"Python 3",
         "name":"python3"
      },
      "language_info":{
         "name":"python",
         "version":"3.7.12",
         "mimetype":"text/x-python",
         "codemirror_mode":{
            "name":"ipython",
            "version":3
         },
         "pygments_lexer":"ipython3",
         "nbconvert_exporter":"python",
         "file_extension":".py"
      }
   },
   "nbformat_minor":4,
   "nbformat":4,
   "cells":[
      {
         "cell_type":"code",
         "source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session",
         "metadata":{
            "_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
            "_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
            "trusted":true
         },
         "execution_count":null,
         "outputs":[]
      },
      {
         "cell_type":"code",
         "source":"import pandas as pd\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.svm import SVC\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.metrics import accuracy_score\nfrom IPython.display import FileLink\n\n# Load the data\ntrain_data = pd.read_csv('/kaggle/input/ms20014/train.csv')\ntest_data = pd.read_csv('/kaggle/input/ms20014/test.csv')\n\n# Extract the features from the text data using TF-IDF vectorizer\nvectorizer = TfidfVectorizer(stop_words='english', lowercase=True, strip_accents='unicode', max_features=5000)\ntrain_features = vectorizer.fit_transform(train_data['text'])\ntest_features = vectorizer.transform(test_data['text'])\n\n# Split the training data into training and validation sets\ntrain_labels = train_data['label']\nX_train, X_val, y_train, y_val = train_test_split(train_features, train_labels, test_size=0.05, random_state=42)\n\n# Define the parameter grid for the SVM model\nparam_grid = {'C': [0.001, 0.01, 0.1, 1, 10, 100],\n              'gamma': [0.001, 0.01, 0.1, 1, 10, 100]}\n\n# Perform grid search to find the best hyperparameters\nsvm = SVC(kernel='rbf', random_state=42)\nsvm_cv = GridSearchCV(svm, param_grid, cv=5)\nsvm_cv.fit(X_train, y_train)\n\n# Make predictions on the validation set and calculate accuracy\ny_pred = svm_cv.predict(X_val)\nacc = accuracy_score(y_val, y_pred)\nprint('Validation accuracy:', acc)\n\n# Make predictions on the test set \ntest_predictions = svm_cv.predict(test_features)\n\n# Save the test predictions to a CSV file\noutput_df = pd.DataFrame({'index': test_data['index'], 'label': test_predictions})\noutput_df.to_csv('_predic.csv', index=False)\n\n# Download the output file\nFileLink(r'_predic.csv')",
         "metadata":{
            "execution":{
               "iopub.status.busy":"2023-04-17T10:46:39.064027Z",
               "iopub.execute_input":"2023-04-17T10:46:39.064523Z",
               "iopub.status.idle":"2023-04-17T10:52:25.128679Z",
               "shell.execute_reply.started":"2023-04-17T10:46:39.064481Z",
               "shell.execute_reply":"2023-04-17T10:52:25.127306Z"
            },
            "trusted":true
         },
         "execution_count":16,
         "outputs":[
            {
               "name":"stdout",
               "text":"Validation accuracy: 0.9333333333333333\n",
               "output_type":"stream"
            },
            {
               "execution_count":16,
               "output_type":"execute_result",
               "data":{
                  "text/plain":"/kaggle/working/_predic.csv",
                  "text/html":"<a href='_predic.csv' target='_blank'>_predic.csv</a><br>"
               },
               "metadata":{}
            }
         ]
      },
      {
         "cell_type":"code",
         "source":"import pandas as pd\nimport nltk\nfrom nltk.corpus import stopwords\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.svm import SVC\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.metrics import accuracy_score\n\n# Download stop words if not already downloaded\nnltk.download('stopwords')\n\n# Load the data\ntrain_data = pd.read_csv('/kaggle/input/ms20014/train.csv')\ntest_data = pd.read_csv('/kaggle/input/ms20014/test.csv')\n\n# Preprocess the text data\nstop_words = stopwords.words('english')\ntrain_data['text'] = train_data['text'].apply(lambda x: ' '.join([word for word in x.lower().split() if word not in stop_words]))\ntest_data['text'] = test_data['text'].apply(lambda x: ' '.join([word for word in x.lower().split() if word not in stop_words]))\n\n# Extract features from the text data using TF-IDF vectorizer\nvectorizer = TfidfVectorizer(lowercase=True, strip_accents='unicode', max_features=5000)\ntrain_features = vectorizer.fit_transform(train_data['text'])\ntest_features = vectorizer.transform(test_data['text'])\n\n# Split the training data into training and validation sets\ntrain_labels = train_data['label']\nX_train, X_val, y_train, y_val = train_test_split(train_features, train_labels, test_size=0.10, random_state=42)\n\n# Define the parameter grid for the SVM model\nparam_grid = {'C': [0.01, 0.1, 1, 10, 100],\n              'gamma': [0.01, 0.1, 1, 10, 100]}\n\n# Perform grid search to find the best hyperparameters\nsvm = SVC(kernel='rbf', random_state=42)\nsvm_cv = GridSearchCV(svm, param_grid, cv=5)\nsvm_cv.fit(X_train, y_train)\n\n# Make predictions on the validation set and calculate accuracy\ny_pred = svm_cv.predict(X_val)\nacc = accuracy_score(y_val, y_pred)\nprint('Validation accuracy:', acc)\n\n# Make predictions on the test set\ntest_predictions = svm_cv.predict(test_features)\n\n# Save the test predictions to a CSV file\noutput_df = pd.DataFrame({'index': test_data['index'], 'label': test_predictions})\noutput_df.to_csv('test_predictions.csv', index=False)",
         "metadata":{
            "execution":{
               "iopub.status.busy":"2023-04-27T14:59:58.272405Z",
               "iopub.execute_input":"2023-04-27T14:59:58.272777Z",
               "iopub.status.idle":"2023-04-27T15:04:02.263078Z",
               "shell.execute_reply.started":"2023-04-27T14:59:58.272740Z",
               "shell.execute_reply":"2023-04-27T15:04:02.261762Z"
            },
            "trusted":true
         },
         "execution_count":3,
         "outputs":[
            {
               "name":"stdout",
               "text":"[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\nValidation accuracy: 0.9333333333333333\n",
               "output_type":"stream"
            }
         ]
      }
   ]
}
